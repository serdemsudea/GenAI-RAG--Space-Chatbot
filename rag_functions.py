import os
from typing import List
from dotenv import load_dotenv
from chromadb import Client, Settings
from chromadb.utils import embedding_functions
from google import genai

# -----------------------------------------------------------
# 0. ORTAM AYARLARI
# -----------------------------------------------------------

# .env'den API anahtarÄ±nÄ± yÃ¼kle
load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")

if not API_KEY:
    raise ValueError("âŒ GEMINI_API_KEY bulunamadÄ±. LÃ¼tfen .env dosyanÄ± kontrol et.")

# Model ve veritabanÄ± sabitleri
EMBEDDING_MODEL = "text-embedding-004"
GENERATION_MODEL = "gemini-2.5-flash"
CHROMA_COLLECTION_NAME = "akbank_rag_collection"
CHROMA_PATH = "chroma_db"


# -----------------------------------------------------------
# 1. GEMINI EMBEDDING SINIFI (ChromaDB Uyumu)
# -----------------------------------------------------------

class GeminiEmbeddingFunction(embedding_functions.EmbeddingFunction):
    """
    Google Gemini'yi kullanarak ChromaDB iÃ§in embedding Ã¼retir.
    Chroma, her __call__ Ã§aÄŸrÄ±sÄ±nda metin listesini gÃ¶nderir.
    """
    def __init__(self, api_key: str, model_name: str = EMBEDDING_MODEL):
        self.client = genai.Client(api_key=api_key)
        self.model_name = model_name

    def __call__(self, texts: List[str]) -> List[List[float]]:
        embeddings = []
        for text in texts:
            try:
                response = self.client.models.embed_content(
                    model=self.model_name,
                    contents=text,
                    task_type="RETRIEVAL_DOCUMENT"
                )

                # Yeni Google SDK bazen dict bazen data objesi dÃ¶ndÃ¼rÃ¼r
                emb = response.get("embedding") if isinstance(response, dict) else response.data[0].embedding
                embeddings.append(emb)
            except Exception as e:
                print(f"âš ï¸ Embedding hatasÄ±: {e}")
                embeddings.append([0.0] * 768)  # BoÅŸ embedding fallback
        return embeddings


# -----------------------------------------------------------
# 2. METÄ°N PARÃ‡ALAMA FONKSÄ°YONU
# -----------------------------------------------------------

def text_splitter(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:
    """
    Tiktoken kullanarak metni belirli bÃ¼yÃ¼klÃ¼kte parÃ§alara ayÄ±rÄ±r.
    """
    from tiktoken import get_encoding
    encoding = get_encoding("cl100k_base")
    tokens = encoding.encode(text)

    chunks = []
    i = 0
    while i < len(tokens):
        chunk_tokens = tokens[i:i + chunk_size]
        chunk_text = encoding.decode(chunk_tokens)
        chunks.append(chunk_text)

        if i + chunk_size >= len(tokens):
            break
        i += chunk_size - chunk_overlap

    return chunks


# -----------------------------------------------------------
# 3. VEKTÃ–R VERÄ°TABANI KURULUMU
# -----------------------------------------------------------

def setup_vector_db(file_path: str):
    """
    TXT dosyasÄ±nÄ± okur, parÃ§alara bÃ¶ler, embedding oluÅŸturur ve ChromaDB'ye kaydeder.
    """
    try:
        print("ğŸ“¦ VektÃ¶r veritabanÄ± kuruluyor...")

        # 1. DosyayÄ± oku
        with open(file_path, "r", encoding="utf-8") as f:
            full_text = f.read()

        # 2. Metni parÃ§alara ayÄ±r
        chunks = text_splitter(full_text)
        if not chunks:
            raise ValueError("Metin parÃ§alanamadÄ± veya dosya boÅŸ.")

        # 3. Chroma Client ve embedding fonksiyonunu baÅŸlat
        client = Client(Settings(persist_directory=CHROMA_PATH))
        gemini_embed_func = GeminiEmbeddingFunction(api_key=API_KEY, model_name=EMBEDDING_MODEL)

        # 4. Koleksiyon oluÅŸtur veya al
        collection = client.get_or_create_collection(
            name=CHROMA_COLLECTION_NAME,
            embedding_function=gemini_embed_func
        )

        # 5. Eksik verileri ekle
        if collection.count() < len(chunks):
            print(f"{len(chunks)} parÃ§a ekleniyor...")
            ids = [f"doc_{i}" for i in range(len(chunks))]
            collection.add(documents=chunks, ids=ids)
            print("âœ… VeritabanÄ± kurulumu tamamlandÄ±.")
        else:
            print("â„¹ï¸ VeritabanÄ± zaten mevcut, yeni veri eklenmedi.")

    except Exception as e:
        print(f"âŒ VeritabanÄ± kurulum hatasÄ±: {e}")
        print("LÃ¼tfen API anahtarÄ±nÄ±zÄ±n (.env) doÄŸru olduÄŸundan ve internet baÄŸlantÄ±nÄ±zÄ±n aktif olduÄŸundan emin olun.")


# -----------------------------------------------------------
# 4. RAG SORGULAMA FONKSÄ°YONU
# -----------------------------------------------------------

def rag_query(query: str) -> str:
    """
    KullanÄ±cÄ± sorgusuna gÃ¶re ChromaDB'den baÄŸlam Ã§eker ve Gemini ile yanÄ±t Ã¼retir.
    """
    try:
        # 1. Client'larÄ± baÅŸlat
        client = Client(Settings(persist_directory=CHROMA_PATH))
        gemini_client = genai.Client(api_key=API_KEY)
        gemini_embed_func = GeminiEmbeddingFunction(api_key=API_KEY, model_name=EMBEDDING_MODEL)

        # 2. Koleksiyonu al
        collection = client.get_collection(
            name=CHROMA_COLLECTION_NAME,
            embedding_function=gemini_embed_func
        )

        # 3. Sorguya en yakÄ±n 3 belgeyi bul
        results = collection.query(query_texts=[query], n_results=3)
        context = "\n---\n".join(results["documents"][0])

        # 4. Prompt hazÄ±rla
        prompt = f"""
        AÅŸaÄŸÄ±daki BAÄLAM'Ä± kullanarak kullanÄ±cÄ± sorusuna kapsamlÄ± bir yanÄ±t ver.
        EÄŸer baÄŸlam yetersizse, "ÃœzgÃ¼nÃ¼m, bu konuda elimde yeterli veri bulunmamaktadÄ±r." de.

        KullanÄ±cÄ± sorusu: {query}

        --- BAÄLAM ---
        {context}
        """

        # 5. YanÄ±t Ã¼ret
        response = gemini_client.models.generate_content(
            model=GENERATION_MODEL,
            contents=prompt,
        )

        return response.text

    except Exception as e:
        return f"âŒ RAG sorgusu sÄ±rasÄ±nda hata oluÅŸtu: {e}"


# -----------------------------------------------------------
# 5. Ã–RNEK KULLANIM
# -----------------------------------------------------------

if __name__ == "__main__":
    # Ã–rnek: veritabanÄ±nÄ± oluÅŸtur
    setup_vector_db("bilgi_kaynagi.txt")  # kendi txt dosyanÄ±n yolunu yaz

    # Ã–rnek sorgu
    cevap = rag_query("Yapay zekanÄ±n bankacÄ±lÄ±ktaki uygulamalarÄ± nelerdir?")
    print("\nğŸ¤– YanÄ±t:\n", cevap)
